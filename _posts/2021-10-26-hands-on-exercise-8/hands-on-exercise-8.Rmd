---
title: "Hands-on Exercise 8"
description: |
  In this hands-on exercise, I learn how to perform geographical segmentation by using appropriate R packages, perform cluster analysis and visualise clustering results.
author:
  - name: Ngah Xin Yan
    url: https://github.com/nxinyan/
date: 10-10-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.retina = 3)
```
# Getting Started

## The Analytical Question

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

## Dataset Used

- **Myanmar Township Boundary Data (i.e. myanmar_township_boundaries)** : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.

- **Shan-ICT.csv**: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.

Both data sets are download from Myanmar Information Management Unit (MIMU)

## Installing and Loading the R packages

The following R packages are used:

- **Spatial data handling**: sf, rgdal and spdep

- **Attribute data handling**: tidyverse, especially readr, ggplot2 and dplyr

- **Choropleth mapping**: tmap

- **Multivariate data visualisation and analysis**: coorplot, ggpubr, and heatmaply

- **Cluster analysis**: cluster

```{r}
packages = c('rgdal', 'spdep', 'tmap', 'sf', 'ggpubr', 'cluster', 'factoextra', 'NbClust', 'heatmaply', 'corrplot', 'psych', 'tidyverse', 'pagedown')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
    }
  library(p,character.only = T)
}
```

# Data Import and Preparation

## Importing geospatial data into R environment

In this section, you will import Myanmar Township Boundary GIS data and its associated attribute table into R environment.

The Myanmar Township Boundary GIS data is in ESRI shapefile format. It will be imported into R environment by using the *st_read()* function of **sf**.

```{r}
shan_sf <- st_read(dsn = "data/geospatial", layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)"))
```

View the content of the newly created shan_sf simple features dataframe

```{r}
shan_sf
```

Use *glimpse()* to reveal the data type of *shan_sf* fields.

```{r}
glimpse(shan_sf)
```

## Importing aspatial data into R environment

The csv file will be import using read_csv function of readr package.

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

The imported InfoComm variables are extracted from **The 2014 Myanmar Population and Housing Census Myanmar**. The attribute data set is called *ict*. It is saved in Râ€™s *tibble data.frame* format.

Reveal the summary statistics of *ict* data.frame.

```{r}
summary(ict)
```

There are a total of eleven fields and 55 observation in the tibble data.frame.

## Derive new variables using **dplyr** package

The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.

Deriving the penetration rate of each ICT variable

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

Review the summary statistics of the newly derived penetration rates

```{r}
summary(ict_derived)
```

There are six new fields that have been added into the data.frame. They are RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR.

# Exploratory Data Analysis (EDA)

## EDA using statistical graphics

Histogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)

```{r}
ggplot(data=ict_derived, aes(x=`RADIO`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data=ict_derived, aes(x=`RADIO`)) +
  geom_boxplot(color="black", fill="light blue")
```

Plotting the distribution of the newly derived variables

```{r}
ggplot(data=ict_derived, aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, color="black", fill="light blue")
```

```{r}
ggplot(data=ict_derived, aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", fill="light blue")
```

***What can you observed from the distributions reveal in the histogram and boxplot?***

The histogram shows that the penetration rate of radio is normally distributed as compared to the number of households owning radios. The boxplot shows that there is only 1 outlier in the penetration rate of radio.

Multiple histograms are plotted to reveal the distribution of the selected variables in the *ict_derived* data.frame.

Create the data visualization. It consist of two main parts. First, creating the individual histograms.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Next, the ggarange() function of ggpubr package is used to group these histograms together.

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

## EDA using choropleth map

### Joining geospatial data with aspatial data

combining both the geospatial data object (i.e. shan_sf) and aspatial data.frame object (i.e. ict_derived) into one using *left_join* function of **dplyr** package.
The *shan_sf* simple feature data.frame will be used as the base data object and the *ict_derived* data.frame will be used as the join table.

```{r}
shan_sf <- left_join(shan_sf, ict_derived, by=c("TS_PCODE"="TS_PCODE"))
```

The code chunk above shows that TS_CODE field is the common field used to perform the left-join.

It is important to note that there is no new output data been created. Instead, the data fields from ict_derived data frame are now updated into the data frame of shan_sf.

### Preparing a choropleth map

Looking at the distribution of Radio penetration rate of Shan State at township level.

To prepare the choropleth by using the *qtm()* function of **tmap** package.

```{r}
qtm(shan_sf, "RADIO_PR")
```

In order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

The choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.

Plot the choropleth maps showing the distribution of total number of households and Radio penetration rate

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

***Can you identify the differences?***

## Correlation Analysis

Ensure that the cluster variables are not highly correlated before performing cluster analysis

Use *corrplot.mixed()* function of **corrplot** package to visualise and analyse the correlation of the input variables.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

# Hierarchy Cluster Analysis

Perform hierarchical cluster analysis. The four major steps are:

## Extrating clustering variables

Extract the clustering variables from the *shan_sf* simple feature object into data.frame.

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

The final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.

Next, we need to change the rows by township name instead of row number

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

Notice that the row number has been replaced into the township name.

Now, we will delete the TS.x field by using the code chunk below.

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## Data Standardisation

In general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.

### Min-Max standardisation

*normalize()* of *heatmaply* package is used to stadardisation the clustering variables by using Min-Max method.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

The values range of the Min-max standardised clustering variables are 0-1 now.

### Z-score standardisation

Z-score standardisation can be performed easily by using scale() of Base R.

Used to stadardise the clustering variables by using Z-score method

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

Notice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.

**Note**: describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation.

Warning: Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.**

### Visualising the standardised clustering variables

It is a good practice to visualise their distribution graphical.

Plot the scaled Radio_PR field.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

The overall distribution of the clustering variables will change after the data standardisation. Hence, it is advisable NOT to perform data standardisation if the values range of the clustering variables are not very large.

## Computing proximity matrix

To calculate distance matrix using *dist()* of R.

*dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is **euclidean** proximity matrix.

To compute the proximity matrix using *euclidean* method

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

To list the content of proxmat for visual inspection.

```{r}
proxmat
```

## Computing hierarchical clustering

*hclust()* employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

Performing hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

Plotting the tree by using plot() of R Graphics.

```{r}
plot(hclust_ward, cex = 0.6)
```

## Selecting the optimal clustering algorithm

One of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use *agnes()* function of **cluster** package. It functions like *hclus()*, however, with the *agnes()* function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

Compute the agglomerative coefficients of all hierarchical clustering algorithms

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

Wardâ€™s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Wardâ€™s method will be used.

## Determining Optimal Clusters

Another technical challenge face by data analyst in performing clustering analysis is to determine the optimal clusters to retain.

There are three commonly used methods to determine the optimal clusters, they are:

- Elbow Method
- Average Silhouette Method
- Gap Statistic Method

### Gap Statistic Method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

To compute the gap statistic, clusGap() of cluster package will be used.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

Next, we can visualise the plot by using fviz_gap_stat() of factoextra package.

```{r}
fviz_gap_stat(gap_stat)
```

With reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

## Interpreting the dendrograms

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually-driven hierarchical clustering analysis

Performing visually-driven hiearchical clustering analysis by using heatmaply package.

Able to build both highly interactive cluster heatmap or static cluster heatmap.

### Transforming the data frame into a matrix

The data was loaded into a data frame, but it has to be a data matrix to make your heatmap.

The code chunk below will be used to transform shan_ict data frame into a data matrix.

```{r}
# shan_ict_mat <- data.matrix(shan_ict)
```

### *Plotting interactive cluster heatmap using heatmaply()*

*heatmaply()* of heatmaply package is used to build an interactive cluster heatmap.

```{r}
# heatmaply(normalize(shan_ict_mat),
#           Colv=NA,
#           dist_method = "euclidean",
#           hclust_method = "ward.D",
#           seriate = "OLO",
#           colors = Blues,
#           k_row = 6,
#           margins = c(NA,200,60,NA),
#           fontsize_row = 4,
#           fontsize_col = 5,
#           main="Geographic Segmentation of Shan State by ICT indicators",
#           xlab = "ICT Indicators",
#           ylab = "Townships of Shan State"
          # )
```

## Mapping the clusters formed

With closed examination of the dendragram above, six clusters are retained. 

*cutree()* of R Base will be used in the code chunk below to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

The output is called *groups*. It is a *list* object.

In order to visualise the clusters, the groups object need to be appended onto *shan_sf* simple feature object.

The code chunk below form the join in three steps:

- the groups list object will be converted into a matrix;

- *cbind()* is used to append groups matrix onto shan_sf to produce an output simple feature object called `shan_sf_cluster`

- rename of *dplyr* package is used to rename *as.matrix.groups* field as *CLUSTER*.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

Next, *qtm()* of **tmap** package is used to plot the choropleth map showing the cluster formed.

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.

# Spatially Constrained Clustering - SKATER approach

Deriving spatially constrained cluster by using *skater()* method of **spdep** package.

## Converting into SpatialPolygonsDataFrame

First, we need to convert shan_sf into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.

The code chunk below uses *as_Spatial()* of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

## Computing Neighbour List

*poly2nd()* of **spdep** package will be used to compute the neighbours list from polygon list.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

We can plot the neighbours list on shan_sp by using the code chunk below. Since we now can plot the community area boundaries as well, we plot this graph on top of the map. The first plot command gives the boundaries. This is followed by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDataFrame (Shan state township boundaries) to extract the centroids of the polygons. These are used as the nodes for the graph representation. We also set the color to blue and specify add=TRUE to plot the network on top of the boundaries.

```{r}
plot(shan_sp, 
     border=grey(.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

Note that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot. In this example, because the boundary map extends further than the graph, we plot it first.

## Computing minimum spanning tree

### Calculating edge costs 

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

### Computing minimum spanning tree

The minimum spanning tree is computed by mean of the mstree() of spdep package

```{r}
shan.mst <- mstree(shan.w)
```

After computing the MST, we can check its class and dimension.

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```

```{r}
head(shan.mst)
```

```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

## Computing spatially constrained clusters using SKATER method

*skater()* of **spdep** package is used to compute the spatially constrained cluster

```{r}
clust6 <- skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

3 mandatory arguments are used by *skater()*: 
- the first columns of the MST matrix
- the data matrix
- the number of cuts

**Note**: It is set to **one less than the number of clusters**. So, the value specified is **not** the number of clusters, but the number of cuts in the graph, one less than the number of clusters.

The result of the skater() is an object of class **skater.** 

```{r}
str(clust6)
```

The most interesting component of this list structure is the groups vector containing the labels of the cluster to which each observation belongs (as before, the label itself is arbitary). This is followed by a detailed summary for each of the clusters in the edges.groups list. Sum of squares measures are given as ssto for the total and ssw to show the effect of each of the cuts on the overall criterion.

Checking cluster assignment

```{r}
ccs6 <- clust6$groups
ccs6
```

```{r}
table(ccs6)
```

Plotting the pruned tree that shows the five clusters on top of the townshop area

```{r}
plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

## Visualising the clusters in choropleth map

Plotting the newly derived clusters

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

Placing both hierarchical clustering and spatially constrained hierarchical clustering maps next to each other for comparison

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```
